main = "Гістограма частоти розподілу")
which.max(data$X18[data$Clust == 0])
# For first cluster
hist(data$X18[data$Clust == 0][-9],breaks = 15, col = "lightblue", xlab = "Residuals",
main = "Гістограма частоти розподілу")
# For first cluster
hist(data$X18[data$Clust == 0][-9],breaks = 10, col = "lightblue", xlab = "Residuals",
main = "Гістограма частоти розподілу")
# For second cluster
hist(data$X18[data$Clust == 1],breaks = 15, col = "lightblue", xlab = "Residuals",
main = "Гістограма частоти розподілу")
# For second cluster
hist(data$X18[data$Clust == 1],breaks = 10, col = "lightblue", xlab = "Residuals",
main = "Гістограма частоти розподілу")
# For first cluster
hist(data$X18[data$Clust == 0],breaks = 10, col = "lightblue", xlab = "X18",
main = "Гістограма частоти розподілу")
# For first cluster
hist(data$X18[data$Clust == 0],breaks = 15, col = "lightblue", xlab = "X18",
main = "Гістограма частоти розподілу")
shapiro.test(data$X18[data$Clust == 0][-9])
# 2. X18 norm
# Without clustering
hist(data$X18,breaks = 10, col = "lightblue", xlab = "X18",
main = "Гістограма частоти розподілу")
# For first cluster
hist(data$X18[data$Clust == 0][-9],breaks = 15, col = "lightblue", xlab = "X18",
main = "Гістограма частоти розподілу")
# For first cluster
hist(data$X18[data$Clust == 0][-9],breaks = 10, col = "lightblue", xlab = "X18",
main = "Гістограма частоти розподілу")
# For second cluster
hist(data$X18[data$Clust == 1],breaks = 10, col = "lightblue", xlab = "X18",
main = "Гістограма частоти розподілу")
# 2. X18 norm
# Without clustering
hist(data$X18,breaks = 15, col = "lightblue", xlab = "X18",
main = "Гістограма частоти розподілу")
hist(data.c1$Y, breaks = 20)
hist(data[data$Clust == 0,]$Y, breaks = 20)
length(data.c1$Y)
shapiro.test(data[data$Clust == 0,]$Y)
shapiro.test(data[data$Clust == 1,]$Y)
shapiro.test(data[data$Clust == 0,][-9,]$Y)
rm(data.scale.for.outliers,nlm.model.X11)
rm(nn.temp,colors,col4)
# data$prev <- c(0,data$Y[1:87]) do not work
nlm.model <- step(lm(data = data, Y ~ .),direction = "both")
# Перевірка моделі 0
summary(nlm.model)
# data$prev <- c(0,data$Y[1:87]) do not work
nlm.model <- step(lm(data = select(data, -Clust, -X24Fact), Y ~ .),direction = "both")
library(dplyr) # робота з тиблицями
library(lmtest) # gqtest на гетероскедастичность, resettest тест Рамсея
library(stringr) # Робота з рядками
library(psych) # look at data
library(randomForest) # For random forest
library(rpart) # For classificarion tree
library(rpart.plot) # For classificarion tree visualization
library(plotly) # 3D Visualization
library(neuralnet) # For neural networks
# data$prev <- c(0,data$Y[1:87]) do not work
nlm.model <- step(lm(data = select(data, -Clust, -X24Fact), Y ~ .),direction = "both")
# Перевірка моделі 0
summary(nlm.model)
nlm.model <- lm(data = data, Y ~ 0 + X3 + X11 + X16 + X18 + X21 + X24)
# Перевірка моделі 0
summary(nlm.model)
nlm.model <- lm(data = data, Y ~ 0 + X3 + X11 + X16 + X18 + X21)
# Перевірка моделі 0
summary(nlm.model)
plot(data$Y, data$X21)
# data$prev <- c(0,data$Y[1:87]) do not work
nlm.model <- step(lm(data = select(data, -Clust, -X24Fact), Y ~ .),direction = "both")
# Перевірка моделі 0
summary(nlm.model)
nlm.model <- lm(data = data, Y ~ 0 + X3 + X11 + X13 + X18 + X24)
# Перевірка моделі 0
summary(nlm.model)
# data$prev <- c(0,data$Y[1:87]) do not work
nlm.model <- step(lm(data = select(data, -Clust, -X24Fact), Y ~ .),direction = "both")
# Перевірка моделі 0
summary(nlm.model)
nlm.model <- lm(data = data, Y ~ 0 + X3 + X11 + X13 + X16 + X18 + X21 + X24)
# Перевірка моделі 0
summary(nlm.model)
nlm.model <- lm(data = data, Y ~ 0 + X3 + X13 + X18 + X21 + X24)
# Перевірка моделі 0
summary(nlm.model)
data$X6 <- data.new$X6
# data$prev <- c(0,data$Y[1:87]) do not work
nlm.model <- step(lm(data = select(data, -Clust, -X24Fact), Y ~ .),direction = "both")
# Перевірка моделі 0
summary(nlm.model)
nlm.model <- lm(data = data, Y ~ 0 + X3 + X11 + X13 + X14 + X16 + X18 + X24)
# Перевірка моделі 0
summary(nlm.model)
mean(data$X3)
summary(data$X3)
plot(data$Y, data$X14)
# nlm.model <- lm(data = data, Y ~ 0 + X3 + X11 + X13 + X18 + X24)
nlm.model.c1 <- step(lm(data = data.c1, Y~.),direction = "both")
# nlm.model <- lm(data = data, Y ~ 0 + X3 + X11 + X13 + X18 + X24)
nlm.model.c1 <- step(lm(data = data[data$Clust == 0,], Y~.),direction = "both")
# Перевірка моделі 1
summary(nlm.model.c1)
# nlm.model <- lm(data = data, Y ~ 0 + X3 + X11 + X13 + X18 + X24)
nlm.model.c1 <- step(lm(data = select(data[data$Clust == 0,], -Clust, -X24Fact), Y~.),direction = "both")
# nlm.model <- lm(data = data, Y ~ 0 + X3 + X11 + X13 + X18 + X24)
nlm.model.c1 <- step(lm(data = select(data[data$Clust == 0,], -Clust, -X24Fact), Y~.),direction = "both")
# Перевірка моделі 0
summary(nlm.model)
# Перевірка моделі 1
summary(nlm.model.c1)
# nlm.model.c1 <- lm(Y ~ X4 + X18 + X21 + X24, data = data[data$Clust == 0,])
nlm.model.c2 <- step(lm(data = select(data[data$Clust == 1,], -Clust, -X24Fact), Y~.),direction = "both")
# Перевірка моделі 2
summary(nlm.model.c2)
nlm.model.c2 <- lm(Y ~ X1 + X3 + X6 + X10 + X13 + X16 + X18 + X21, data = data[data$Clust == 1,])
# Перевірка моделі 2
summary(nlm.model.c2)
nlm.model.c2 <- lm(Y ~ X1 + X3 + X10 + X13 + X16 + X18, data = data[data$Clust == 1,])
# Перевірка моделі 2
summary(nlm.model.c2)
plot(data$X6, data$Y)
# Перевірка моделі 1
summary(nlm.model.c1)
# Перевірка моделі 0
summary(nlm.model)
data.temp <- data
which.max(data.temp$X16)
max(data.temp$X16)
mean(data.temp$X16)
data.temp$X16[which.max(data.temp$X16)]
data.temp$X16[which.max(data.temp$X16)] <- 600
nlm.model <- lm(data = data.temp, Y ~ 0 + X3 + X11 + X13 + X14 + X16 + X18 + X24)
# Перевірка моделі 0
summary(nlm.model)
nlm.model <- lm(data = data.temp, Y ~ 0 + X3 + X11 + X13 + X18 + X24)
# Перевірка моделі 0
summary(nlm.model)
nlm.model <- lm(data = data, Y ~ 0 + X3 + X11 + X13 + X18 + X24)
# Перевірка моделі 0
summary(nlm.model)
# data$prev <- c(0,data$Y[1:87]) do not work
nlm.model <- step(lm(data = select(data, -Clust, -X24Fact), Y ~ .),direction = "both")
# Перевірка моделі 0
summary(nlm.model)
confint(nlm.model)
nlm.model <- lm(data = data, Y ~ 0 + X3 + X11 + X13 + X18 + X24)
# Перевірка моделі 0
summary(nlm.model)
AIC(nlm.model)
BIC(nlm.model)
AIC(nlm.model.c1)
AIC(nlm.model.c2)
# Перевірка моделі 2
summary(nlm.model.c2)
hist(nlm.model$residuals,  col = "blue")
hist(nlm.model$residuals,  col = "green")
shapiro.test(nlm.model$residuals)
plot(nlm.model)
acf(nlm.model$residuals)
resettest(nlm.model)
# Librarys -------------------------------------------------
library(outliers) # grubbs.test
library(NbClust) # klast
library(flexclust) # k-means with manchetten
library(corrplot)
library(car) # vif + leveneTest + outlierTest
library(ggplot2) # Графіка
library(cvTools) # cross-validation
library(FWDselect) # selection
resettest(nlm.model)
install.packages("fRegression")
library(fRegression) # For resettest!
resettest(nlm.model)
remove.packages("fRegression")
library(FWDselect) # selection
install.packages("FWDselect")
library(FWDselect) # selection
resettest(nlm.model)
install.packages("lmtest")
library(lmtest) # gqtest на гетероскедастичность, resettest тест Рамсея
resettest(nlm.model)
rnorm(20, 1, 10)
mean(rnorm(20, 1, 10))
shapiro.test(nlm.model$residuals)
resettest(nlm.model)
# Перевірка моделі 0
summary(nlm.model)
summary(select(data, X3, X11, X13, X18, X24))
cor(data$X6, data$X3)
cor(data$X13, data$X3)
# Перевірка моделі 1
summary(nlm.model.c1)
# Перевірка моделі 1
summary(nlm.model.c1)
# Перевірка моделі 2
summary(nlm.model.c2)
summary(select(data, X1, X4, X21))
plotcp(tree.model)
printcp(tree.model)
predict(tree.model, select(data, -Clust, -Y, -X18)
)
predict(tree.model, select(data, -Clust, -Y, -X18, -X6, -X24Fact), type = "vector")
select(data, -Clust, -Y, -X18, -X6, -X24Fact)
# 1) ---
qplot(predict(tree.model, select(data, -Clust, -Y, -X18, -X6), type = "vector"),
predict(tree.model, select(data, -Clust, -Y, -X18), type = "vector") - data$Y)
cor(predict(tree.model, select(data, -Clust, -Y, -X18), type = "vector"), data$Y)^2
cor(data$Y, predict(tree.model.clust, select(data, -Y), type = "vector"))^2
# With Clusters ---
tree.model.clust <- rpart(Y~.,data = select(data, -X18, -X24, -X6) ,method = "anova",
control = rpart.control(minsplit = 10, minbucket = 5, maxdepth = 6))
rpart.plot(tree.model.clust)
cor(data$Y, predict(tree.model, select(data, -Clust, -Y, -X18), type = "vector"))^2
rpart.plot(tree.model)
plotcp(tree.model)
residuals.metod.train <- predict(tree.model, select(data.train, -Clust, -Y), type = "vector") - data.train$Y
residuals.metod.test <- predict(tree.model, select(data.test, -Clust, -Y), type = "vector") - data.test$Y
hist(residuals.metod.train, breaks = 20, col = "lightgreen",
xlab = "Residuals", freq = F,
main = str_c("Shapiro test, p-value = ",
as.character(shapiro.test(residuals.metod.train)[2])))
rm(list = c("residuals.metod.train","residuals.metod.test"))
hist(residuals.tree, breaks = 20, col = "lightgreen",
xlab = "Residuals", freq = F,
main = str_c("Shapiro test, p-value = ",
as.character(shapiro.test(residuals.metod.train)[2])))
residuals.tree <- predict(tree.model, select(data, -Clust, -Y, -X6), type = "vector") - data$Y
hist(residuals.tree, breaks = 20, col = "lightgreen",
xlab = "Residuals", freq = F,
main = str_c("Shapiro test, p-value = ",
as.character(shapiro.test(residuals.metod.train)[2])))
hist(residuals.tree, breaks = 20, col = "lightgreen",
xlab = "Residuals", freq = F,
main = str_c("Shapiro test, p-value = ",
as.character(shapiro.test(residuals.tree)[2])))
hist(residuals.tree, breaks = 10, col = "lightgreen",
xlab = "Residuals", freq = F,
main = str_c("Shapiro test, p-value = ",
as.character(shapiro.test(residuals.tree)[2])))
lines(density(residuals.tree), col = "red", lwd = 2)
hist(residuals.tree, breaks = 10, col = "grey",
xlab = "Residuals", freq = F,
main = str_c("Shapiro test, p-value = ",
as.character(shapiro.test(residuals.tree)[2])))
hist(residuals.tree, breaks = 10, col = "yellow",
xlab = "Residuals", freq = F,
main = str_c("Shapiro test, p-value = ",
as.character(shapiro.test(residuals.tree)[2])))
lines(density(residuals.tree), col = "red", lwd = 2)
# 1) ---
qplot(predict(tree.model, select(data, -Clust, -Y, -X18, -X6), type = "vector"),
predict(tree.model, select(data, -Clust, -Y, -X18), type = "vector") - data$Y)
# 1) ---
qplot(predict(tree.model, select(data, -Clust, -Y, -X18, -X6), type = "vector"),
predict(tree.model, select(data, -Clust, -Y, -X18), type = "vector") - data$Y,
xlab = "Predicted Values", ylab = "Residuals", col = "blue")
# 1) ---
qplot(predict(tree.model, select(data, -Clust, -Y, -X18, -X6), type = "vector"),
predict(tree.model, select(data, -Clust, -Y, -X18), type = "vector") - data$Y,
xlab = "Predicted Values", ylab = "Residuals", col = "black")
# 1) ---
qplot(predict(tree.model, select(data, -Clust, -Y, -X18, -X6), type = "vector"),
predict(tree.model, select(data, -Clust, -Y, -X18), type = "vector") - data$Y,
xlab = "Predicted Values", ylab = "Residuals", col = "1")
# 1) ---
qplot(predict(tree.model, select(data, -Clust, -Y, -X18, -X6), type = "vector"),
predict(tree.model, select(data, -Clust, -Y, -X18), type = "vector") - data$Y,
xlab = "Predicted Values", ylab = "Residuals") + stat_smooth(method = "lm")
acf(predict(tree.model, select(data.train, -Clust, -X18, -Y, -X6), type = "vector") - data.train$Y)
acf(predict(tree.model, select(data, -Clust, -X18, -Y, -X6), type = "vector") - data.train$Y)
predict(tree.model, select(data, -Clust, -X18, -Y, -X6), type = "vector")
acf(predict(tree.model, select(data, -Clust, -X18, -Y, -X6), type = "vector") - data$Y)
acf(predict(tree.model, select(data, -Clust, -X18, -Y, -X6), type = "vector") - data$Y,
main = "Autocr")
acf(predict(tree.model, select(data, -Clust, -X18, -Y, -X6), type = "vector") - data$Y,
main = "Autocrelation function")
cor(data$Y, predict(tree.model, select(data, -Clust, -Y, -X18), type = "vector"))^2
rpart.plot(tree.model)
rpart.plot(tree.model.clust)
cor(data$Y, predict(tree.model.clust, select(data, -Y), type = "vector"))^2
# Побудова моделі
CV <- create.CV(folds = cvFolds(nrow(data), K = 10, R = 9),
cost = "rtmspe", trim = 0.1,eps.L = 0.001)
data[,-c(15,length(data))]
library(cvTools)
plot(nn.best)
cor(predict.net, data.test.scale$Y)^2
cor(nn.best$net.result[[1]], data.train.scale$Y)^2
plot(1:88, data.scale$Y, type = "l")
lines(1:88,neuralnet::compute(nn.best,select(data.scale,-Y))$net.result, col = "red")
data.scale <- as.data.frame(scale(data, center = sapply(data, min),
scale = sapply(data, max) - sapply(data, min)))
plot(1:88, data.scale$Y, type = "l")
lines(1:88,neuralnet::compute(nn.best,select(data.scale,-Y))$net.result, col = "red")
data.scale <- as.data.frame(scale(select(data, -X6, -X24Fact, -Clust), center = sapply(data, min),
scale = sapply(data, max) - sapply(data, min)))
data.scale <- select(data, -X6, -X24Fact, -Clust)
data.scale <- as.data.frame(scale(data.scale, center = sapply(data.scale, min),
scale = sapply(data.scale, max) - sapply(data.scale, min)))
plot(1:88, data.scale$Y, type = "l")
lines(1:88,neuralnet::compute(nn.best,select(data.scale,-Y))$net.result, col = "red")
select(data.scale,-Y)
compute(nn.best,select(data.scale,-Y))
data.scale <- select(data, -X6, -X24Fact)
data.scale <- as.data.frame(scale(data.scale, center = sapply(data.scale, min),
scale = sapply(data.scale, max) - sapply(data.scale, min)))
plot(1:88, data.scale$Y, type = "l")
lines(1:88,neuralnet::compute(nn.best,select(data.scale,-Y))$net.result, col = "red")
rf.model$ntree
floor(sqrt(ncol(data.train)))
ncol(data.train)
cor(rf.model$predicted, data.train$Y)^2
cor(predict.rf, data.test$Y)^2
rf.model.new <- randomForest(x = as.data.frame(cbind(select(data.train, -Y), Nlm = nlm.model$fitted.values[-index])), y = data.train$Y,
mtry = floor(sqrt(ncol(data.train))), ntree = 1000, nodesize = 10,
do.trace = 1000/20, keep.forest = T, replace = F, importance = T,
localImp = F, proximity = F, norm.votes = T, corr.bias = F, keep.inbag = F)
cor(rf.mode.newl$predicted, data.train$Y)^2
cor(rf.mode.new$predicted, data.train$Y)^2
cor(rf.model.new$predicted, data.train$Y)^2
predict.rf.new <- predict(rf.model.new, newdata = as.data.frame(cbind(select(data.test, -Y), Nlm = nlm.model$fitted.values[index])))
cor(predict.rf.new, data.test$Y)^2
rf.model.new <- randomForest(x = as.data.frame(cbind(select(data.train, -Y), Nlm = nlm.model$fitted.values[-index])), y = data.train$Y,
mtry = floor(sqrt(ncol(data.train))), ntree = 1000, nodesize = 5,
do.trace = 1000/20, keep.forest = T, replace = F, importance = T,
localImp = F, proximity = F, norm.votes = T, corr.bias = F, keep.inbag = F)
predict.rf.new <- predict(rf.model.new, newdata = as.data.frame(cbind(select(data.test, -Y), Nlm = nlm.model$fitted.values[index])))
cor(rf.model.new$predicted, data.train$Y)^2
cor(predict.rf.new, data.test$Y)^2
rf.model.new <- randomForest(x = as.data.frame(cbind(select(data.train, -Y), Nlm = nlm.model$fitted.values[-index])), y = data.train$Y,
mtry = floor(sqrt(ncol(data.train))), ntree = 1000, nodesize = 5,
do.trace = 1000/20, keep.forest = T, replace = T, importance = T,
localImp = F, proximity = F, norm.votes = T, corr.bias = F, keep.inbag = F)
predict.rf.new <- predict(rf.model.new, newdata = as.data.frame(cbind(select(data.test, -Y), Nlm = nlm.model$fitted.values[index])))
cor(rf.model.new$predicted, data.train$Y)^2
cor(predict.rf.new, data.test$Y)^2
rf.model.new <- randomForest(x = as.data.frame(cbind(select(data.train, -Y), Nlm = nlm.model$fitted.values[-index])), y = data.train$Y,
mtry = floor(sqrt(ncol(data.train))), ntree = 1000, nodesize = 10,
do.trace = 1000/20, keep.forest = T, replace = T, importance = T,
localImp = F, proximity = F, norm.votes = T, corr.bias = F, keep.inbag = F)
predict.rf.new <- predict(rf.model.new, newdata = as.data.frame(cbind(select(data.test, -Y), Nlm = nlm.model$fitted.values[index])))
cor(rf.model.new$predicted, data.train$Y)^2
cor(predict.rf.new, data.test$Y)^2
rf.model.new <- randomForest(x = as.data.frame(cbind(select(data.train, -Y), Nlm = nlm.model$fitted.values[-index])), y = data.train$Y,
mtry = floor(sqrt(ncol(data.train))), ntree = 1000, nodesize = 3,
do.trace = 1000/20, keep.forest = T, replace = T, importance = T,
localImp = F, proximity = F, norm.votes = T, corr.bias = F, keep.inbag = F)
predict.rf.new <- predict(rf.model.new, newdata = as.data.frame(cbind(select(data.test, -Y), Nlm = nlm.model$fitted.values[index])))
cor(rf.model.new$predicted, data.train$Y)^2
cor(predict.rf.new, data.test$Y)^2
set.seed(3217)
rf.model.new <- randomForest(x = as.data.frame(cbind(select(data.train, -Y), Nlm = nlm.model$fitted.values[-index])), y = data.train$Y,
mtry = floor(sqrt(ncol(data.train))), ntree = 1000, nodesize = 10,
do.trace = 1000/20, keep.forest = T, replace = T, importance = T,
localImp = F, proximity = F, norm.votes = T, corr.bias = F, keep.inbag = F)
predict.rf.new <- predict(rf.model.new, newdata = as.data.frame(cbind(select(data.test, -Y), Nlm = nlm.model$fitted.values[index])))
cor(rf.model.new$predicted, data.train$Y)^2
cor(predict.rf.new, data.test$Y)^2
rf.model.new <- randomForest(x = as.data.frame(cbind(select(data.train, -Y), Nlm = nlm.model$fitted.values[-index])), y = data.train$Y,
mtry = floor(sqrt(ncol(data.train))), ntree = 1000, nodesize = 10,
do.trace = 1000/20, keep.forest = T, replace = T, importance = T,
localImp = F, proximity = F, norm.votes = T, corr.bias = F, keep.inbag = F)
predict.rf.new <- predict(rf.model.new, newdata = as.data.frame(cbind(select(data.test, -Y), Nlm = nlm.model$fitted.values[index])))
cor(rf.model.new$predicted, data.train$Y)^2
cor(predict.rf.new, data.test$Y)^2
set.seed(32173)
rf.model.new <- randomForest(x = as.data.frame(cbind(select(data.train, -Y), Nlm = nlm.model$fitted.values[-index])), y = data.train$Y,
mtry = floor(sqrt(ncol(data.train))), ntree = 1000, nodesize = 10,
do.trace = 1000/20, keep.forest = T, replace = T, importance = T,
localImp = F, proximity = F, norm.votes = T, corr.bias = F, keep.inbag = F)
predict.rf.new <- predict(rf.model.new, newdata = as.data.frame(cbind(select(data.test, -Y), Nlm = nlm.model$fitted.values[index])))
cor(rf.model.new$predicted, data.train$Y)^2
cor(predict.rf.new, data.test$Y)^2
set.seed(32173)
rf.model.new <- randomForest(x = as.data.frame(cbind(select(data.train, -Y), Nlm = nlm.model$fitted.values[-index])), y = data.train$Y,
mtry = floor(sqrt(ncol(data.train))), ntree = 1000, nodesize = 1,
do.trace = 1000/20, keep.forest = T, replace = T, importance = T,
localImp = F, proximity = F, norm.votes = T, corr.bias = F, keep.inbag = F)
predict.rf.new <- predict(rf.model.new, newdata = as.data.frame(cbind(select(data.test, -Y), Nlm = nlm.model$fitted.values[index])))
cor(rf.model.new$predicted, data.train$Y)^2
cor(predict.rf.new, data.test$Y)^2
cor(rf.model$predicted, data.train$Y)^2
cor(predict.rf, data.test$Y)^2
rm(rf.model.new,predict.rf.new)
cor(rf.model$predicted, data.train$Y)^2
cor(predict.rf, data.test$Y)^2
plot(1:88, c(data.train$Y, data.test$Y), type = "l")
lines(1:88,c(rf.model$predicted, predict.rf), col = "red")
plot(1:88, data$Y, type = "l")
lines(1:88,predict(rf.model, newdata = as.data.frame(cbind(select(data, -Y, -X6, -X24Fact),
Nlm = nlm.model$fitted.values))), col = "red")
varImpPlot(rf.model)
cor(predict.xgboost, data.test$Y)^2
cor(predict(xgboost.model,cbind(as.matrix(select(data.train,-Y)),
nlm.model$fitted.values[-index])), data.train$Y)^2
data.test$Y
predict.xgboost
or(predict.xgboost, data.test$Y)
cor(predict.xgboost, data.test$Y)^2
cor(predict(xgboost.model,cbind(as.matrix(select(data.train,-Y)),
nlm.model$fitted.values[-index])), data.train$Y)^2
xgboost.model.new <- xgboost(data = cbind(as.matrix(select(data.train,-Y)),
nlm.model$fitted.values[-index]), label = data.train$Y,
objective = "reg:linear", eval_metric = "rmse", max.depth = 10,
eta = 0.0005, nrounds = 10000, subsample = 1, colsample_bytree = 0.7,
nthread = 3)
# For XGBoost ---
library(xgboost)
library(methods)
library(data.table)
xgboost.model.new <- xgboost(data = cbind(as.matrix(select(data.train,-Y)),
nlm.model$fitted.values[-index]), label = data.train$Y,
objective = "reg:linear", eval_metric = "rmse", max.depth = 10,
eta = 0.0005, nrounds = 10000, subsample = 1, colsample_bytree = 0.7,
nthread = 3)
predict.xgboost.new <- predict(xgboost.model.new, cbind(as.matrix(select(data.test,-Y)),
nlm.model$fitted.values[index]))
cor(predict.xgboost.new, data.test$Y)^2
cor(predict(xgboost.model.new,cbind(as.matrix(select(data.train,-Y)),
nlm.model$fitted.values[-index])), data.train$Y)^2
xgboost.model.new <- xgboost(data = cbind(as.matrix(select(data.train,-Y)),
nlm.model$fitted.values[-index]), label = data.train$Y,
objective = "reg:linear", eval_metric = "rmse", max.depth = 5,
eta = 0.0005, nrounds = 10000, subsample = 1, colsample_bytree = 0.7,
nthread = 3)
predict.xgboost.new <- predict(xgboost.model.new, cbind(as.matrix(select(data.test,-Y)),
nlm.model$fitted.values[index]))
cor(predict.xgboost.new, data.test$Y)^2
cor(predict(xgboost.model.new,cbind(as.matrix(select(data.train,-Y)),
nlm.model$fitted.values[-index])), data.train$Y)^2
xgboost.model.new <- xgboost(data = cbind(as.matrix(select(data.train,-Y)),
nlm.model$fitted.values[-index]), label = data.train$Y,
objective = "reg:linear", eval_metric = "rmse", max.depth = 3,
eta = 0.0005, nrounds = 10000, subsample = 1, colsample_bytree = 0.7,
nthread = 3)
predict.xgboost.new <- predict(xgboost.model.new, cbind(as.matrix(select(data.test,-Y)),
nlm.model$fitted.values[index]))
cor(predict.xgboost.new, data.test$Y)^2
cor(predict(xgboost.model.new,cbind(as.matrix(select(data.train,-Y)),
nlm.model$fitted.values[-index])), data.train$Y)^2
xgboost.model.new <- xgboost(data = cbind(as.matrix(select(data.train,-Y)),
nlm.model$fitted.values[-index]), label = data.train$Y,
objective = "reg:linear", eval_metric = "rmse", max.depth = 2,
eta = 0.0005, nrounds = 10000, subsample = 1, colsample_bytree = 0.7,
nthread = 3)
predict.xgboost.new <- predict(xgboost.model.new, cbind(as.matrix(select(data.test,-Y)),
nlm.model$fitted.values[index]))
cor(predict.xgboost.new, data.test$Y)^2
cor(predict(xgboost.model.new,cbind(as.matrix(select(data.train,-Y)),
nlm.model$fitted.values[-index])), data.train$Y)^2
cor(predict.xgboost, data.test$Y)^2
xgboost.model.new <- xgboost(data = cbind(as.matrix(select(data.train,-Y)),
nlm.model$fitted.values[-index]), label = data.train$Y,
objective = "reg:linear", eval_metric = "rmse", max.depth = 1,
eta = 0.0005, nrounds = 10000, subsample = 1, colsample_bytree = 0.7,
nthread = 3)
predict.xgboost.new <- predict(xgboost.model.new, cbind(as.matrix(select(data.test,-Y)),
nlm.model$fitted.values[index]))
cor(predict.xgboost, data.test$Y)^2
cor(predict.xgboost.new, data.test$Y)^2
cor(predict(xgboost.model.new,cbind(as.matrix(select(data.train,-Y)),
nlm.model$fitted.values[-index])), data.train$Y)^2
xgboost.model <- xgboost.model.new
predict.xgboost <- predict(xgboost.model, cbind(as.matrix(select(data.test,-Y)),
nlm.model$fitted.values[index]))
cor(predict.xgboost, data.test$Y)^2
cor(predict(xgboost.model,cbind(as.matrix(select(data.train,-Y)),
nlm.model$fitted.values[-index])), data.train$Y)^2
rm(xgboost.model.new, predict.xgboost.new)
cor(predict.xgboost, data.test$Y)^2
cor(predict(xgboost.model,cbind(as.matrix(select(data.train,-Y)),
nlm.model$fitted.values[-index])), data.train$Y)^2
plot(1:88, data$Y, type = "l")
lines(1:88,predict(xgboost.model, newdata = as.data.frame(cbind(select(data, -Y, -X6, -X24Fact),
Nlm = nlm.model$fitted.values))), col = "red")
lines(1:88,predict(xgboost.model, newdata = as.matrix(cbind(select(data, -Y, -X6, -X24Fact),
Nlm = nlm.model$fitted.values))), col = "red")
lines(1:88,predict(xgboost.model, newdata = cbind(select(data, -Y, -X6, -X24Fact),
Nlm = nlm.model$fitted.values)), col = "red")
lines(1:88,predict(xgboost.model, newdata = as.matrix(cbind(select(data, -Y, -X6, -X24Fact),
Nlm = nlm.model$fitted.values))), col = "red")
lines(1:88,predict(xgboost.model, newdata = as.matrix(cbind(select(data, -Y),
Nlm = nlm.model$fitted.values))), col = "red")
lines(1:88,predict(xgboost.model, newdata = as.matrix(cbind(select(data, -Y),
nlm.model$fitted.values))), col = "red")
predict.xgboost <- predict(xgboost.model, cbind(as.matrix(select(data.test,-Y)),
nlm.model$fitted.values[index]))
lines(1:88,predict(xgboost.model, newdata = cbind(as.matrix(select(data,-Y)),
nlm.model$fitted.values)), col = "red")
cbind(as.matrix(select(data,-Y)),
nlm.model$fitted.values))
cbind(as.matrix(select(data,-Y)),
nlm.model$fitted.values)
xgboost.model <- xgboost(data = cbind(as.matrix(select(data.train,-Y,-X24Fact)),
nlm.model$fitted.values[-index]), label = data.train$Y,
objective = "reg:linear", eval_metric = "rmse", max.depth = 1,
eta = 0.0005, nrounds = 10000, subsample = 1, colsample_bytree = 0.7,
nthread = 3)
cbind(as.matrix(select(data,-Y,-X24Fact,-X6)),
nlm.model$fitted.values))
cbind(as.matrix(select(data,-Y,-X24Fact,-X6)),
nlm.model$fitted.values)
lines(1:88,predict(xgboost.model, newdata = cbind(as.matrix(select(data,-Y,-X24Fact,-X6)),
nlm.model$fitted.values)), col = "red")
plot(1:88, data$Y, type = "l")
lines(1:88, nlm.model$fitted.values)
plot(1:88, data$Y, type = "l")
lines(1:88, nlm.model$fitted.values, col = "red")
